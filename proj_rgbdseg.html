<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>RGBD Segmentation</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width">
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Huazhu Fu</h1>
<p>
<small>huazhufu (AT) gmail (DOT) com </small><br><br>
<a href="https://github.com/HzFu" target="_blank">[GitHub]</a>  
<a href="http://dblp.uni-trier.de/pers/hd/f/Fu:Huazhu" target="_blank">[DBLP]</a>  <br>
<a href="http://scholar.google.com.sg/citations?user=jCvUBYMAAAAJ" target="_blank">[Google Scholar]</a> </p> <br>
<p class="view"><a href="index.html">Homepage</a></p>
<p class="view"><a href="sub_publication.html">Publications</a></p>
<p class="view"><a href="sub_codes.html">Codes and Data</a></p>
<p class="view"><a href="sub_projects.html">Projects</a></p>
      </header>

      <section>

<h2>
<a id="project_title" class="anchor" href="#project_title" aria-hidden="true"><span class="octicon octicon-link"></span></a>Object-based RGBD foreground segmentation</h2>




<h4>
<a id="Introduction-page" class="anchor" href="#Introduction-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction:</h4>

<p>We present an object-based co-segmentation method that takes advantage of depth data and is able to correctly handle noisy images in which the common foreground object is missing. With RGBD images, our method utilizes the depth channel to enhance identification of similar foreground objects via a proposed RGBD co-saliency map, as well as to improve detection of object-like regions and provide depth- based local features for region comparison. To accurately deal with noisy images where the common object appears more than or less than once, we formulate co-segmentation in a fully-connected graph structure together with mutual exclusion (mutex) constraints that prevent improper solutions. Experiments show that this object-based RGBD co-segmentation with mutex constraints outperforms related techniques on an RGBD co-segmentation dataset, while effectively processing noisy images. Moreover, we show that this method also provides performance comparable to state- of-the-art RGB co-segmentation techniques on regular RGB images with depth maps estimated from them.</p>

<div style="text-align: center; display: block; margin-right: auto;">
<img src="sub_img/rgbd_seg_framework.jpg" border="0" width="600"><br></div><br>


<hr />
<h4>Paper:</h4>
    
<p>[1] Huazhu Fu, Dong Xu, Stephen Lin, Jiang Liu, <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Fu_Object-Based_RGBD_Image_2015_CVPR_paper.html" target="_blank"><strong>"Object-based RGBD Image Co-segmentation with Mutex Constraint"</strong></a> in <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2015, pp. 4428-4436. <br>

[2] Huazhu Fu, Dong Xu, Stephen Lin, <strong>"Object-based Multiple Foreground Segmentation in RGBD Video"</strong>, in <em>IEEE Transactions on Image Processing (TIP)<em>, 2017, Accepted. <br> <a href="http://dx.doi.org/10.1109/TIP.2017.2651369" target="_blank">[PDF]</a>
</p>

<hr />
<h4>RGBD image co-segmentation dataset:</h4>
<p> We build a RGBD image co-segmentation dataset, which contains 16 image sets, each of 6 to 17 images taken from indoor scenes with one common foreground object (193 images in total): RGBD image co-segmentation dataset (~102MB) <a href="https://onedrive.live.com/redir?resid=F3A8A31ABFAC51B0!269&authkey=!AHUBN0lk5kQLWzQ&ithint=file%2czip" target="_blank"><font color="#ff0000">[Download Link]</font></a>, and our result (~20MB) <a href="https://onedrive.live.com/redir?resid=F3A8A31ABFAC51B0!255&authkey=!ANuwbADPp9Dsm4Q&ithint=file%2czip"><font color="#ff0000">[Download Link]</font></a>.</p>

<h4>RGBD video foreground segmentation dataset:</h4>
<p> We also build a RGBD video foreground segmentation dataset for journal version, which contains 11 videos, each of 20 to 56 frames with one or two foregrounds: RGBD video segmentation dataset and our result (~332MB) <a href="https://1drv.ms/u/s!ArBRrL8ao6jzhmX6SC75hVJppb3D" target="_blank"><font color="#ff0000">[Download Link]</font></a>.</p>

<hr />
<h4>Related Works:</h4>

<p>[1] Huazhu Fu, Dong Xu, Bao Zhang, Stephen Lin, Rabab K. Ward, <a href="http://dx.doi.org/10.1109/TIP.2015.2442915" target="_blank"><strong>"Object-based Multiple Foreground Video Co-segmentation via Multi-state Selection Graph"</strong></a>, <em>IEEE Transactions on Image Processing (TIP)</em>, vol. 24, no. 11, pp. 3415-3424, 2015.  
[<a href="https://github.com/HzFu/VideoCoSeg_MSG" target="_blank"><font color="#ff0000">Code</font></a>] [<a href="proj_video_coseg.html" target="_blank"><font color="#ff0000">Project</font></a>] </p>

<p>[2] Huazhu Fu, Xiaochun Cao, Zhuowen Tu, <a href="http://dx.doi.org/10.1109%2FTIP.2013.2260166" target="_blank"><strong>"Cluster-based Co-saliency Detection"</strong></a>, <em>IEEE Transactions on Image Processing (TIP)</em>, vol. 22, no. 10, pp. 3766-3778, 2013.  
[<a href="https://github.com/HzFu/Cosaliency_tip2013" target="_blank"><font color="#ff0000">Code</font></a>] </p>

<p>[3] Xiaochun Cao, Zhiqiang Tao, Bao Zhang, Huazhu Fu, Wei Feng, <a href="http://dx.doi.org/10.1109/TIP.2014.2332399" target="_blank"><strong>"Self-adaptively Weighted Co-saliency Detection via Rank Constraint"</strong></a>, <em>IEEE Transactions on Image Processing (TIP)</em>, vol. 23, no. 9, pp. 4175-4186, 2014. 
[<a href="https://github.com/HzFu/SACS_TIP2014" target="_blank"><font color="#ff0000">Code</font></a>]</p>

      </section>

    </div>
    <script src="../../javascripts/scale.fix.js"></script>
  </body>
</html>
